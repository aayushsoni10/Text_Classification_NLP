{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mEjkQQHLct7",
        "colab_type": "code",
        "outputId": "c6a6e345-1115-482d-e1a4-a140c65d2685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras import utils as np_utils\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek57Je9npBXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('final_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-b4atPsZFCp",
        "colab_type": "code",
        "outputId": "ca12a73c-16f0-4ffa-c6c4-ce76b8874b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df['desc'] = [word.lower() for word in df['desc']]\n",
        "print(df['desc'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0         \"humira gave me my life back.  i was sicker th...\n",
            "1         \"i&#039;m having a terrible time with the hysi...\n",
            "2         \"i have been on four different birth control p...\n",
            "3         \"pain relief is limited and barely noticed. i ...\n",
            "4         \"aside from the out of pocket cost ($130.00 fo...\n",
            "5         \"i love this medication  i have been taking it...\n",
            "6         \"i read several reviews about this drug and wa...\n",
            "7         \"be careful when taking this medication. i too...\n",
            "8         \"age 33 male\\r\\r\\ni have adhd and high functio...\n",
            "9         \"i originally started out on solely prozac for...\n",
            "10        \"i started travatan z two weeks ago after pres...\n",
            "11        \"i&#039;ve suffered from excessive sweat under...\n",
            "12        \"this is my second month on this pill, about t...\n",
            "13        \"i have been taking loestrin 24 for 3 months. ...\n",
            "14        \"i started celexa 20mg about 2 months ago for ...\n",
            "15        \"paxil has changed my life for the better. i a...\n",
            "16        \"soma helps my back pain effectively.i seem to...\n",
            "17        \"this is my first month on loestrin.  i was pr...\n",
            "18        \"yes it&#039;s effective, however, the taste i...\n",
            "19        \"worked wonders, it was great to finally find ...\n",
            "20        \"i am rxed this 6/day for neuropathy but on th...\n",
            "21        \"my experience of this medicine.. only made me...\n",
            "22        \"i was on cymbalta for about 3 months.  i can&...\n",
            "23        \"i have taken 4 of these pills. the first one ...\n",
            "24        \"i am on 2.4mg. the egg burps and diarrhea are...\n",
            "25        \"i&#039;ve been freaking out recently, because...\n",
            "26        \"i started rogaine and propecia about a year a...\n",
            "27        \"fall 2015 noticed b/p was staying in the 140/...\n",
            "28                             \"helps in-between flareups.\"\n",
            "29        \"i was prescribed this medicine for frequent m...\n",
            "                                ...                        \n",
            "159493    \"i&#039;m reading a lot of bad reviews on here...\n",
            "159494    \"controlled itchiness but did little to reduce...\n",
            "159495    \"i first stumbled upon this medication while l...\n",
            "159496    \"i took ella one approximately 8-9 hours after...\n",
            "159497    \"good stuff, the committee in my head has gone...\n",
            "159498    \"i take as needed .5mg, more if needed. works ...\n",
            "159499    \"i&#039;ve been given zofran pre op and post o...\n",
            "159500    \"i had the paragard inserted around may 2015 a...\n",
            "159501    \"i was on cyclessa no problems, until my dr. s...\n",
            "159502    \"my 5 year old son was diagnosed yesterday wit...\n",
            "159503    \"i thought it helped a lot, not a miracle or a...\n",
            "159504    \"i am a 24 year old male with adhd, high funct...\n",
            "159505    \"this medicine is wonderful.  i never believed...\n",
            "159506    \"hopefully my experience will give some with n...\n",
            "159507    \"i was diagnosed with fibromyalgia ten years a...\n",
            "159508    \"initially, my doctor started me out on 20mg, ...\n",
            "159509    \"i have suffered with panic attacks all my lif...\n",
            "159510    \"i have had quite remarkable relief of severe ...\n",
            "159511    \"my personal experience.\\r\\n  *   effectivenes...\n",
            "159512    \"i just went to my doctor and had my second mi...\n",
            "159513    \"pristiq has helped me more than any other ant...\n",
            "159514    \"i am a 70 yr old woman &amp; have had anxiety...\n",
            "159515    \"i started taking this medicine when i was 24 ...\n",
            "159516    \"i switched to this pill because i was on spri...\n",
            "159517    \"i have been taking cardura for 15 years along...\n",
            "159518    \"i&#039;m 33 years old and have no reason to b...\n",
            "159519    \"my periods are shorter and i&#039;m regular. ...\n",
            "159520    \"drinking this is what comes to mind if someon...\n",
            "159521    \"i was diagnosed with psoriatic arthritis in m...\n",
            "159522    \"my family has a history of perspiration probl...\n",
            "Name: desc, Length: 159523, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5-pJ_-rrDL3",
        "colab_type": "code",
        "outputId": "604b5160-78fd-441b-ac44-2e5ce8ac1d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = df['desc']\n",
        "Y = df['rating']\n",
        "vocab_size = 15000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_matrix(X, mode='tfidf')\n",
        "print(X.shape)\n",
        "Y = np_utils.to_categorical(Y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159523, 15000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlNrY7pzmmum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adyEnsnjhwGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('dataX.npy', X)\n",
        "np.save('dataY.npy', Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIRThyrgnQU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.load('dataX.npy')\n",
        "Y = np.load('dataY.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYI2tx8er4xC",
        "colab_type": "code",
        "outputId": "fb491576-ae16-42c6-cfd4-f9b7b3977524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(15000,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(11))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1024)              15361024  \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 11)                11275     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 11)                0         \n",
            "=================================================================\n",
            "Total params: 16,421,899\n",
            "Trainable params: 16,421,899\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFXI8myWr_Ar",
        "colab_type": "code",
        "outputId": "38508be4-0027-4acd-d40f-21c5ac924e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG4nfVhttARH",
        "colab_type": "code",
        "outputId": "d8f41cba-0c2e-401e-d74a-fb67dfb1eb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, Y, batch_size=256, epochs=125, verbose=1, validation_split=0.0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/125\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "159523/159523 [==============================] - 34s 214us/step - loss: 1.5308 - acc: 0.4677\n",
            "Epoch 2/125\n",
            "159523/159523 [==============================] - 29s 179us/step - loss: 0.8032 - acc: 0.7319\n",
            "Epoch 3/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.2897 - acc: 0.9052\n",
            "Epoch 4/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.1735 - acc: 0.9434\n",
            "Epoch 5/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.1416 - acc: 0.9548\n",
            "Epoch 6/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.1193 - acc: 0.9621\n",
            "Epoch 7/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.1057 - acc: 0.9663\n",
            "Epoch 8/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0963 - acc: 0.9700\n",
            "Epoch 9/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0863 - acc: 0.9728\n",
            "Epoch 10/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0757 - acc: 0.9766\n",
            "Epoch 11/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0751 - acc: 0.9767\n",
            "Epoch 12/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0745 - acc: 0.9775\n",
            "Epoch 13/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0669 - acc: 0.9797\n",
            "Epoch 14/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0608 - acc: 0.9817\n",
            "Epoch 15/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0600 - acc: 0.9819\n",
            "Epoch 16/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0575 - acc: 0.9828\n",
            "Epoch 17/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0552 - acc: 0.9835\n",
            "Epoch 18/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0500 - acc: 0.9849\n",
            "Epoch 19/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0506 - acc: 0.9848\n",
            "Epoch 20/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0526 - acc: 0.9847\n",
            "Epoch 21/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0478 - acc: 0.9857\n",
            "Epoch 22/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0469 - acc: 0.9863\n",
            "Epoch 23/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0435 - acc: 0.9873\n",
            "Epoch 24/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0439 - acc: 0.9871\n",
            "Epoch 25/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0409 - acc: 0.9881\n",
            "Epoch 26/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0414 - acc: 0.9879\n",
            "Epoch 27/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0399 - acc: 0.9884\n",
            "Epoch 28/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0417 - acc: 0.9884\n",
            "Epoch 29/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0386 - acc: 0.9891\n",
            "Epoch 30/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0371 - acc: 0.9896\n",
            "Epoch 31/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0371 - acc: 0.9897\n",
            "Epoch 32/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0368 - acc: 0.9898\n",
            "Epoch 33/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0380 - acc: 0.9897\n",
            "Epoch 34/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0349 - acc: 0.9903\n",
            "Epoch 35/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0341 - acc: 0.9906\n",
            "Epoch 36/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0333 - acc: 0.9909\n",
            "Epoch 37/125\n",
            "159523/159523 [==============================] - 28s 176us/step - loss: 0.0331 - acc: 0.9909\n",
            "Epoch 38/125\n",
            "159523/159523 [==============================] - 28s 176us/step - loss: 0.0341 - acc: 0.9907\n",
            "Epoch 39/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0362 - acc: 0.9902\n",
            "Epoch 40/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0316 - acc: 0.9911\n",
            "Epoch 41/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0326 - acc: 0.9912\n",
            "Epoch 42/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0320 - acc: 0.9911\n",
            "Epoch 43/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0324 - acc: 0.9913\n",
            "Epoch 44/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0292 - acc: 0.9917\n",
            "Epoch 45/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0317 - acc: 0.9916\n",
            "Epoch 46/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0328 - acc: 0.9914\n",
            "Epoch 47/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0306 - acc: 0.9920\n",
            "Epoch 48/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0301 - acc: 0.9919\n",
            "Epoch 49/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0305 - acc: 0.9920\n",
            "Epoch 50/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0287 - acc: 0.9925\n",
            "Epoch 51/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0298 - acc: 0.9923\n",
            "Epoch 52/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0293 - acc: 0.9922\n",
            "Epoch 53/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0289 - acc: 0.9924\n",
            "Epoch 54/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0323 - acc: 0.9920\n",
            "Epoch 55/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0293 - acc: 0.9925\n",
            "Epoch 56/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0280 - acc: 0.9929\n",
            "Epoch 57/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0295 - acc: 0.9927\n",
            "Epoch 58/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0306 - acc: 0.9923\n",
            "Epoch 59/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0294 - acc: 0.9925\n",
            "Epoch 60/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0305 - acc: 0.9923\n",
            "Epoch 61/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0293 - acc: 0.9929\n",
            "Epoch 62/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0272 - acc: 0.9929\n",
            "Epoch 63/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0267 - acc: 0.9935\n",
            "Epoch 64/125\n",
            "159523/159523 [==============================] - 27s 172us/step - loss: 0.0268 - acc: 0.9931\n",
            "Epoch 65/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0259 - acc: 0.9936\n",
            "Epoch 66/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0282 - acc: 0.9932\n",
            "Epoch 67/125\n",
            "159523/159523 [==============================] - 28s 172us/step - loss: 0.0289 - acc: 0.9930\n",
            "Epoch 68/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0287 - acc: 0.9930\n",
            "Epoch 69/125\n",
            "159523/159523 [==============================] - 27s 172us/step - loss: 0.0281 - acc: 0.9931\n",
            "Epoch 70/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0268 - acc: 0.9933\n",
            "Epoch 71/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0280 - acc: 0.9934\n",
            "Epoch 72/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0263 - acc: 0.9936\n",
            "Epoch 73/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0273 - acc: 0.9935\n",
            "Epoch 74/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0260 - acc: 0.9938\n",
            "Epoch 75/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0257 - acc: 0.9937\n",
            "Epoch 76/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0271 - acc: 0.9936\n",
            "Epoch 77/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0249 - acc: 0.9941\n",
            "Epoch 78/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0269 - acc: 0.9937\n",
            "Epoch 79/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0287 - acc: 0.9933\n",
            "Epoch 80/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0277 - acc: 0.9936\n",
            "Epoch 81/125\n",
            "159523/159523 [==============================] - 28s 176us/step - loss: 0.0245 - acc: 0.9942\n",
            "Epoch 82/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0256 - acc: 0.9940\n",
            "Epoch 83/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0246 - acc: 0.9942\n",
            "Epoch 84/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0267 - acc: 0.9937\n",
            "Epoch 85/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0264 - acc: 0.9939\n",
            "Epoch 86/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0256 - acc: 0.9940\n",
            "Epoch 87/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0260 - acc: 0.9940\n",
            "Epoch 88/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0246 - acc: 0.9941\n",
            "Epoch 89/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0256 - acc: 0.9942\n",
            "Epoch 90/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0260 - acc: 0.9942\n",
            "Epoch 91/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0258 - acc: 0.9943\n",
            "Epoch 92/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0260 - acc: 0.9941\n",
            "Epoch 93/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0272 - acc: 0.9940\n",
            "Epoch 94/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0282 - acc: 0.9940\n",
            "Epoch 95/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0240 - acc: 0.9945\n",
            "Epoch 96/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0266 - acc: 0.9941\n",
            "Epoch 97/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0267 - acc: 0.9941\n",
            "Epoch 98/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0244 - acc: 0.9944\n",
            "Epoch 99/125\n",
            "159523/159523 [==============================] - 28s 172us/step - loss: 0.0258 - acc: 0.9943\n",
            "Epoch 100/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0253 - acc: 0.9943\n",
            "Epoch 101/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0257 - acc: 0.9942\n",
            "Epoch 102/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0273 - acc: 0.9939\n",
            "Epoch 103/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0254 - acc: 0.9944\n",
            "Epoch 104/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0273 - acc: 0.9942\n",
            "Epoch 105/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0258 - acc: 0.9944\n",
            "Epoch 106/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0254 - acc: 0.9945\n",
            "Epoch 107/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0249 - acc: 0.9944\n",
            "Epoch 108/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0257 - acc: 0.9945\n",
            "Epoch 109/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0256 - acc: 0.9944\n",
            "Epoch 110/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0247 - acc: 0.9946\n",
            "Epoch 111/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0222 - acc: 0.9947\n",
            "Epoch 112/125\n",
            "159523/159523 [==============================] - 27s 172us/step - loss: 0.0241 - acc: 0.9947\n",
            "Epoch 113/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0260 - acc: 0.9948\n",
            "Epoch 114/125\n",
            "159523/159523 [==============================] - 28s 176us/step - loss: 0.0245 - acc: 0.9948\n",
            "Epoch 115/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0245 - acc: 0.9945\n",
            "Epoch 116/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0251 - acc: 0.9946\n",
            "Epoch 117/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0235 - acc: 0.9947\n",
            "Epoch 118/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0263 - acc: 0.9946\n",
            "Epoch 119/125\n",
            "159523/159523 [==============================] - 28s 174us/step - loss: 0.0225 - acc: 0.9951\n",
            "Epoch 120/125\n",
            "159523/159523 [==============================] - 28s 173us/step - loss: 0.0245 - acc: 0.9947\n",
            "Epoch 121/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0244 - acc: 0.9946\n",
            "Epoch 122/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0236 - acc: 0.9949\n",
            "Epoch 123/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0249 - acc: 0.9947\n",
            "Epoch 124/125\n",
            "159523/159523 [==============================] - 28s 175us/step - loss: 0.0244 - acc: 0.9949\n",
            "Epoch 125/125\n",
            "159523/159523 [==============================] - 28s 176us/step - loss: 0.0236 - acc: 0.9950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0192f5b7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGeEFm209nzK",
        "colab_type": "code",
        "outputId": "2a531622-ea8b-4570-ed12-89488930ac9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"model_2016A7PS0720G.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}